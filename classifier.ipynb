{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/device:GPU:0\n",
      "Train on 13168 samples\n",
      "Epoch 1/16\n",
      "13168/13168 [==============================] - 53s 4ms/sample - loss: 7.3030 - accuracy: 0.5961\n",
      "Epoch 2/16\n",
      "13168/13168 [==============================] - 104s 8ms/sample - loss: 6.6028 - accuracy: 0.7651\n",
      "Epoch 3/16\n",
      "13168/13168 [==============================] - 99s 8ms/sample - loss: 7.6593 - accuracy: 0.8018\n",
      "Epoch 4/16\n",
      "13168/13168 [==============================] - 94s 7ms/sample - loss: 7.1173 - accuracy: 0.8283\n",
      "Epoch 5/16\n",
      "13168/13168 [==============================] - 86s 7ms/sample - loss: 4.7809 - accuracy: 0.8510\n",
      "Epoch 6/16\n",
      "13168/13168 [==============================] - 83s 6ms/sample - loss: 2.3294 - accuracy: 0.8799\n",
      "Epoch 7/16\n",
      "13168/13168 [==============================] - 92s 7ms/sample - loss: 1.3525 - accuracy: 0.8909\n",
      "Epoch 8/16\n",
      "13168/13168 [==============================] - 111s 8ms/sample - loss: 1.1348 - accuracy: 0.8884\n",
      "Epoch 9/16\n",
      "13168/13168 [==============================] - 120s 9ms/sample - loss: 1.0888 - accuracy: 0.8863\n",
      "Epoch 10/16\n",
      "13168/13168 [==============================] - 77s 6ms/sample - loss: 0.9953 - accuracy: 0.8919\n",
      "Epoch 11/16\n",
      "13168/13168 [==============================] - 79s 6ms/sample - loss: 0.8948 - accuracy: 0.8961\n",
      "Epoch 12/16\n",
      "13168/13168 [==============================] - 85s 6ms/sample - loss: 0.7727 - accuracy: 0.9067\n",
      "Epoch 13/16\n",
      "13168/13168 [==============================] - 66s 5ms/sample - loss: 0.6003 - accuracy: 0.9332\n",
      "Epoch 14/16\n",
      "13168/13168 [==============================] - 86s 7ms/sample - loss: 0.4338 - accuracy: 0.9554\n",
      "Epoch 15/16\n",
      "13168/13168 [==============================] - 69s 5ms/sample - loss: 0.2928 - accuracy: 0.9825\n",
      "Epoch 16/16\n",
      "13168/13168 [==============================] - 67s 5ms/sample - loss: 0.2405 - accuracy: 0.9918\n",
      "accuracy 0.9426229508196722\n",
      "average f1-score: 0.9423758849528252\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "gpu_device_name = tf.test.gpu_device_name()\n",
    "print(gpu_device_name)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_excel('1617241934831197.xlsx', sheet_name = 'All', header = 0, names = ['content', 'channelName', 'title'], keep_default_na = False, engine = 'openpyxl').astype(str)\n",
    "indices = dataset.loc[dataset['content'] == ''].index.values\n",
    "dataset.drop(indices, axis = 0)\n",
    "\n",
    "dataset['news'] = dataset['title'] + '!' + dataset['content']\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import jieba\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "process = lambda x: list(jieba.cut(x))\n",
    "dataset['tokenized'] = dataset['news'].apply(process)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(dataset['tokenized'])\n",
    "vocab = tokenizer.word_index\n",
    "\n",
    "kind = { '财经': 0, '房产': 1, '教育': 2, '科技': 3, '军事': 4, '汽车': 5, '体育': 6, '综合体育最新': 6, '体育焦点': 6, '游戏': 7, '娱乐': 8 }\n",
    "label = []\n",
    "\n",
    "for channel in dataset['channelName']:\n",
    "    label.append(kind[channel])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataset['tokenized'], label, test_size = 0.1)\n",
    "x_train_word_ids = tokenizer.texts_to_sequences(x_train)\n",
    "x_test_word_ids = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "x_train_padded_seqs = pad_sequences(x_train_word_ids, maxlen = 128)\n",
    "x_test_padded_seqs = pad_sequences(x_test_word_ids, maxlen = 128)\n",
    "\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('model/baike_26g_news_13g_novel_229g.bin', binary = True)\n",
    "embedding_matrix = np.zeros((len(vocab) + 1, 128))\n",
    "\n",
    "for word, i in vocab.items():\n",
    "    try:\n",
    "        embedding_vector = model[str(word)]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        continue\n",
    "        \n",
    "from tensorflow.keras import layers, optimizers, metrics, regularizers\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "def mish(x):\n",
    "    return x * K.tanh(K.softplus(x))\n",
    "\n",
    "def my_loss(y_true, y_pred, e = 0.1):\n",
    "    loss1 = K.categorical_crossentropy(y_true, y_pred)\n",
    "    loss2 = K.categorical_crossentropy(K.ones_like(y_pred) / 9, y_pred)\n",
    "    loss = (1 - e) * loss1 + e * loss2\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def cosine_decay_with_warmup(global_step, learning_rate_base, total_steps, warmup_learning_rate = 0.0, warmup_steps = 0, hold_base_rate_steps = 0):\n",
    "    learning_rate = 0.5 * learning_rate_base * (1 + np.cos(np.pi * (global_step - warmup_steps - hold_base_rate_steps) / float(total_steps - warmup_steps - hold_base_rate_steps)))\n",
    "    \n",
    "    if hold_base_rate_steps > 0:\n",
    "        learning_rate = np.where(global_step > warmup_steps + hold_base_rate_steps, learning_rate, learning_rate_base)\n",
    "        \n",
    "    slope = (learning_rate_base - warmup_learning_rate) / warmup_steps\n",
    "    warmup_rate = slope * global_step + warmup_learning_rate\n",
    "    learning_rate = np.where(global_step < warmup_steps, warmup_rate, learning_rate)\n",
    "    \n",
    "    return np.where(global_step > total_steps, 0.0, learning_rate)\n",
    "\n",
    "class WarmUpCosineDecayScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, learning_rate_base, total_steps, global_step_init = 0, warmup_learning_rate = 0.0, warmup_steps = 0, hold_base_rate_steps = 0, verbose = 0):\n",
    "        super(WarmUpCosineDecayScheduler, self).__init__()\n",
    "        self.learning_rate_base = learning_rate_base\n",
    "        self.total_steps = total_steps\n",
    "        self.global_step = global_step_init\n",
    "        self.warmup_learning_rate = warmup_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.hold_base_rate_steps = hold_base_rate_steps\n",
    "        self.verbose = verbose\n",
    "        self.learning_rates = []\n",
    "        \n",
    "    def on_batch_end(self, batch, logs = None):\n",
    "        self.global_step = self.global_step + 1\n",
    "        lr = K.get_value(self.model.optimizer.lr)\n",
    "        self.learning_rates.append(lr)\n",
    "        \n",
    "    def on_batch_begin(self, batch, logs = None):\n",
    "        lr = cosine_decay_with_warmup(global_step = self.global_step, learning_rate_base = self.learning_rate_base, total_steps = self.total_steps, warmup_learning_rate = self.warmup_learning_rate, warmup_steps = self.warmup_steps, hold_base_rate_steps = self.hold_base_rate_steps)\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "        \n",
    "def text_cnn(x_train_padded_seqs, y_train, x_test_padded_seqs, y_test, embedding_matrix):\n",
    "    inlet = keras.Input(shape = (128,), dtype = 'float64')\n",
    "    embedder = layers.Embedding(len(vocab) + 1, 128, input_length = 128, weights = [embedding_matrix], trainable = False)\n",
    "    embed = embedder(inlet)\n",
    "\n",
    "    cnn1 = layers.Conv1D(256, 3, padding = 'same', strides = 1, kernel_regularizer = regularizers.l2(0.01))(embed)\n",
    "    cnn1 = layers.BatchNormalization()(cnn1)\n",
    "    cnn1 = layers.Activation(mish)(cnn1)\n",
    "    cnn1 = layers.MaxPooling1D(pool_size = 32)(cnn1)\n",
    "    cnn2 = layers.Conv1D(256, 4, padding = 'same', strides = 1, kernel_regularizer = regularizers.l2(0.01))(embed)\n",
    "    cnn2 = layers.BatchNormalization()(cnn2)\n",
    "    cnn2 = layers.Activation(mish)(cnn2)\n",
    "    cnn2 = layers.MaxPooling1D(pool_size = 31)(cnn2)\n",
    "    cnn3 = layers.Conv1D(256, 5, padding = 'same', strides = 1, kernel_regularizer = regularizers.l2(0.01))(embed)\n",
    "    cnn3 = layers.BatchNormalization()(cnn3)\n",
    "    cnn3 = layers.Activation(mish)(cnn3)\n",
    "    cnn3 = layers.MaxPooling1D(pool_size = 30)(cnn3)\n",
    "    \n",
    "    cnn = K.concatenate([cnn1, cnn2, cnn3], axis = -1)\n",
    "    \n",
    "    flat = layers.Flatten()(cnn)\n",
    "    drop = layers.Dropout(0.5)(flat)\n",
    "    outlet = layers.Dense(9, activation = 'softmax')(drop)\n",
    "    model = keras.Model(inputs = inlet, outputs = outlet)\n",
    "    opt = optimizers.SGD(lr = 0.001, momentum = 0.9, decay = 0.0, nesterov = True)\n",
    "    model.compile(loss = my_loss, optimizer = opt, metrics = ['accuracy'])\n",
    "    plot_model(model, to_file = 'model/model.png', show_shapes = True, show_layer_names = False)\n",
    "    \n",
    "    sample_count = 13168\n",
    "    epochs = 16\n",
    "    warmup_epoch = 8\n",
    "    batch_size = 16\n",
    "    learning_rate_base = 0.01\n",
    "    total_steps = int(epochs * sample_count / batch_size)\n",
    "    warmup_steps = int(warmup_epoch * sample_count / batch_size)\n",
    "    warmup_batches = warmup_epoch * sample_count / batch_size\n",
    "    \n",
    "    warm_up_lr = WarmUpCosineDecayScheduler(learning_rate_base = learning_rate_base, total_steps = total_steps, warmup_learning_rate = 4e-06, warmup_steps = warmup_steps, hold_base_rate_steps = 5)\n",
    "    \n",
    "    one_hot_labels = keras.utils.to_categorical(y_train, num_classes = 9)\n",
    "    model.fit(x_train_padded_seqs, one_hot_labels, batch_size = 16, epochs = 16, callbacks = [warm_up_lr])\n",
    "    model.save('model/textcnn.h5')\n",
    "    result = model.predict(x_test_padded_seqs) \n",
    "    result_labels = np.argmax(result, axis = 1)\n",
    "    # y_predict = list(map(str, result_labels))\n",
    "    # print('accuracy', sklearn.metrics.accuracy_score(y_test, y_predict))\n",
    "    # print('average f1-score:', sklearn.metrics.f1_score(y_test, y_predict, average = 'weighted'))\n",
    "    print('accuracy', sklearn.metrics.accuracy_score(y_test, result_labels))\n",
    "    print('average f1-score:', sklearn.metrics.f1_score(y_test, result_labels, average = 'weighted'))\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    text_cnn(x_train_padded_seqs, y_train, x_test_padded_seqs, y_test, embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
